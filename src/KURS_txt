!pip install highway-env

import gymnasium as gym
import highway_env

import scipy.linalg
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import namedtuple

import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
from tqdm.notebook import trange

from pathlib import Path
import sys
from tqdm.notebook import trange
!pip install tensorboardx gym pyvirtualdisplay
!apt-get install -y xvfb ffmpeg
!git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null
sys.path.insert(0, '/content/HighwayEnv/scripts/')
from utils import record_videos, show_videos

Transition = namedtuple('Transition', ['state', 'action', 'next_state'])

def collect_interaction_data(env, size=2000, action_repeat=2):
    data, done = [], True
    for _ in trange(size, desc="Collecting interaction data"):
        action = env.action_space.sample()
        for _ in range(action_repeat):
            if done:
              previous_obs, info = env.reset()
            obs, reward, done, truncated, info = env.step(action)
            data.append(Transition(torch.Tensor(previous_obs["observation"]),
                                   torch.Tensor(action),
                                   torch.Tensor(obs["observation"])))
            previous_obs = obs
    return data

env = gym.make("parking-v0")
data = collect_interaction_data(env)
print("Sample transition:", data[0])

class DynamicsModel(nn.Module):
    STATE_X = 0
    STATE_Y = 1

    def __init__(self, state_size, action_size, hidden_size, dt):
        super().__init__()
        self.state_size, self.action_size, self.dt = state_size, action_size, dt
        A_size, B_size = state_size * state_size, state_size * action_size
        self.A1 = nn.Linear(state_size + action_size, hidden_size)
        self.A2 = nn.Linear(hidden_size, A_size)
        self.B1 = nn.Linear(state_size + action_size, hidden_size)
        self.B2 = nn.Linear(hidden_size, B_size)

    def forward(self, x, u):
        """
            Predict x_{t+1} = f(x_t, u_t)
        :param x: a batch of states
        :param u: a batch of actions
        """
        xu = torch.cat((x, u), -1)
        xu[:, self.STATE_X:self.STATE_Y+1] = 0  # Remove dependency in (x,y)
        A = self.A2(F.relu(self.A1(xu)))
        A = torch.reshape(A, (x.shape[0], self.state_size, self.state_size))
        B = self.B2(F.relu(self.B1(xu)))
        B = torch.reshape(B, (x.shape[0], self.state_size, self.action_size))
        dx = A @ x.unsqueeze(-1) + B @ u.unsqueeze(-1)
        return x + dx.squeeze()*self.dt


dynamics = DynamicsModel(state_size=env.observation_space.spaces["observation"].shape[0],
                         action_size=env.action_space.shape[0],
                         hidden_size=64,
                         dt=1/env.unwrapped.config["policy_frequency"])
print("Forward initial model on a sample transition:", dynamics(data[0].state.unsqueeze(0),
                                                                data[0].action.unsqueeze(0)).detach())

optimizer = torch.optim.Adam(dynamics.parameters(), lr=0.01)

# Split dataset into training and validation
train_ratio = 0.7
train_data, validation_data = data[:int(train_ratio * len(data))], \
                              data[int(train_ratio * len(data)):]

def compute_loss(model, data_t, loss_func = torch.nn.MSELoss()):
    states, actions, next_states = data_t
    predictions = model(states, actions)
    return loss_func(predictions, next_states)

def transpose_batch(batch):
    return Transition(*map(torch.stack, zip(*batch)))

def train(model, train_data, validation_data, epochs=1500):
    train_data_t = transpose_batch(train_data)
    validation_data_t = transpose_batch(validation_data)
    losses = np.full((epochs, 2), np.nan)
    for epoch in trange(epochs, desc="Train dynamics"):
        # Compute loss gradient and step optimizer
        loss = compute_loss(model, train_data_t)
        validation_loss = compute_loss(model, validation_data_t)
        losses[epoch] = [loss.detach().numpy(), validation_loss.detach().numpy()]
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    # Plot losses
    plt.plot(losses)
    plt.yscale("log")
    plt.xlabel("epochs")
    plt.ylabel("loss")
    plt.legend(["train", "validation"])
    plt.show()

train(dynamics, train_data, validation_data)

def predict_trajectory(state, actions, model, action_repeat=1):
    states = []
    for action in actions:
        for _ in range(action_repeat):
            state = model(state, action)
            states.append(state)
    return torch.stack(states, dim=0)

def plot_trajectory(states, color):
    scales = np.array(env.unwrapped.config["observation"]["scales"])
    states = np.clip(states.squeeze(1).detach().numpy() * scales, -100, 100)
    plt.plot(states[:, 0], states[:, 1], color=color, marker='.')
    plt.arrow(states[-1,0], states[-1,1], states[-1,4]*1, states[-1,5]*1, color=color)

def visualize_trajectories(model, state, horizon=15):
    plt.cla()
    # Draw a car
    plt.plot(state.numpy()[0]+2.5*np.array([-1, -1, 1, 1, -1]),
             state.numpy()[1]+1.0*np.array([-1, 1, 1, -1, -1]), 'k')
    # Draw trajectories
    state = state.unsqueeze(0)
    colors = iter(plt.get_cmap("tab20").colors)
    # Generate commands
    for steering in np.linspace(-0.5, 0.5, 3):
        for acceleration in np.linspace(0.8, 0.4, 2):
            actions = torch.Tensor([acceleration, steering]).view(1,1,-1)
            # Predict trajectories
            states = predict_trajectory(state, actions, model, action_repeat=horizon)
            plot_trajectory(states, color=next(colors))
    plt.axis("equal")
    plt.show()

visualize_trajectories(dynamics, state=torch.Tensor([0, 0, 0, 0, 1, 0]))

def reward_model(states, goal, gamma=None):
    """
        The reward is a weighted L1-norm between the state and a goal
    :param Tensor states: a batch of states. shape: [batch_size, state_size].
    :param Tensor goal: a goal state. shape: [state_size].
    :param float gamma: a discount factor
    """
    goal = goal.expand(states.shape)
    reward_weigths = torch.Tensor(env.unwrapped.config["reward_weights"])
    rewards = -torch.pow(torch.norm((states-goal)*reward_weigths, p=1, dim=-1), 0.5)
    if gamma:
        time = torch.arange(rewards.shape[0], dtype=torch.float).unsqueeze(-1).expand(rewards.shape)
        rewards *= torch.pow(gamma, time)
    return rewards

obs, info = env.reset()
print("Reward of a sample transition:", reward_model(torch.Tensor(obs["observation"]).unsqueeze(0),
                                                     torch.Tensor(obs["desired_goal"])))

def cem_planner(state, goal, action_size, horizon=5, population=100, selection=10, iterations=5):
    state = state.expand(population, -1)
    action_mean = torch.zeros(horizon, 1, action_size)
    action_std = torch.ones(horizon, 1, action_size)
    for _ in range(iterations):
        # 1. Draw sample sequences of actions from a normal distribution
        actions = torch.normal(mean=action_mean.repeat(1, population, 1), std=action_std.repeat(1, population, 1))
        actions = torch.clamp(actions, min=env.action_space.low.min(), max=env.action_space.high.max())
        states = predict_trajectory(state, actions, dynamics, action_repeat=5)
        # 2. Fit the distribution to the top-k performing sequences
        returns = reward_model(states, goal).sum(dim=0)
        _, best = returns.topk(selection, largest=True, sorted=False)
        best_actions = actions[:, best, :]
        action_mean = best_actions.mean(dim=1, keepdim=True)
        action_std = best_actions.std(dim=1, unbiased=False, keepdim=True)
    return action_mean[0].squeeze(dim=0)


# Run the planner on a sample transition
action = cem_planner(torch.Tensor(obs["observation"]),
                     torch.Tensor(obs["desired_goal"]),
                     env.action_space.shape[0])
print("Planned action:", action)

def mpc_planner(dynamics, state, goal, horizon=10, max_iters=10, alpha=0.1):
    n_state = state.shape[0]
    n_action = env.action_space.shape[0]

    actions = torch.zeros(horizon, n_action, requires_grad=True)

    optimizer = torch.optim.Adam([actions], lr=alpha)

    for i in range(max_iters):
        states = [state]
        for t in range(horizon):
            next_state = dynamics(states[-1].unsqueeze(0), actions[t].unsqueeze(0)).squeeze(0)
            states.append(next_state)

        states = torch.stack(states)
        cost = -reward_model(states, goal).sum()

        optimizer.zero_grad()
        cost.backward()
        optimizer.step()

        with torch.no_grad():
            actions.clamp_(min=env.action_space.low.min(), max=env.action_space.high.max())

        if cost.item() < 1e-3:
            break

    return actions[0].detach()

# Run the planner on a sample transition
obs, info = env.reset()
action = mpc_planner(dynamics, torch.Tensor(obs["observation"]),
                     torch.Tensor(obs["desired_goal"]))
print("Planned action:", action)

import torch
import torch.nn.functional as F

def old_ilqr_planner(state, goal, dynamics_model, action_size, state_size, horizon=15, iterations=20, learning_rate=0.05):
    action_sequence = torch.zeros(horizon, action_size, requires_grad=True)
    def compute_cost(states, goal):
        reward_weights = torch.Tensor(env.unwrapped.config["reward_weights"])
        cost = torch.norm((states - goal) * reward_weights, p=1, dim=-1)
        return cost
    optimizer = torch.optim.Adam([action_sequence], lr=learning_rate)
    for _ in range(iterations):
        optimizer.zero_grad()
        current_state = state.unsqueeze(0)
        states = [current_state]
        for t in range(horizon):
            action = action_sequence[t].unsqueeze(0)
            next_state = dynamics_model(current_state, action)
            states.append(next_state)
            current_state = next_state
        states = torch.cat(states, dim=0)
        total_cost = compute_cost(states.squeeze(1), goal).sum()
        total_cost.backward()
        optimizer.step()
        with torch.no_grad():
            action_sequence.data = torch.clamp(action_sequence, min=env.action_space.low.min(), max=env.action_space.high.max())
    return action_sequence[0].detach()

# Run the planner on a sample transition
action = ilqr_planner(torch.Tensor(obs["observation"]),
                      torch.Tensor(obs["desired_goal"]),
                      dynamics,
                      env.action_space.shape[0],
                      env.observation_space.spaces["observation"].shape[0])
print("Planned action:", action)

import torch
import torch.nn.functional as F

def ilqr_planner(state, goal, dynamics_model, action_size, state_size, horizon=20, iterations=15, learning_rate=0.02):
    action_sequence = torch.zeros(horizon, action_size, requires_grad=True)
    def compute_cost(states, goal):
        reward_weights = torch.Tensor(env.unwrapped.config["reward_weights"])
        cost = torch.norm((states - goal) * reward_weights, p=1, dim=-1)
        return cost
    optimizer = torch.optim.Adam([action_sequence], lr=learning_rate)
    for _ in range(iterations):
        optimizer.zero_grad()
        current_state = state.unsqueeze(0)
        states = [current_state]
        for t in range(horizon):
            action = action_sequence[t].unsqueeze(0)
            next_state = dynamics_model(current_state, action)
            states.append(next_state)
            current_state = next_state
        states = torch.cat(states, dim=0)
        total_cost = compute_cost(states.squeeze(1), goal).sum()
        total_cost.backward()
        optimizer.step()
        with torch.no_grad():
            action_sequence.data = torch.clamp(action_sequence, min=env.action_space.low.min(), max=env.action_space.high.max())
    return action_sequence[0].detach()

# Run the planner on a sample transition
action = ilqr_planner(torch.Tensor(obs["observation"]),
                      torch.Tensor(obs["desired_goal"]),
                      dynamics,
                      env.action_space.shape[0],
                      env.observation_space.spaces["observation"].shape[0])
print("Planned action:", action)

import time
import matplotlib.pyplot as plt
from tqdm import trange

def test_planners(planners, env, num_tests=5, num_steps=100):
    results = []

    for planner_name, planner in planners.items():
        for test_num in range(num_tests):
            obs, info = env.reset()
            initial_state = torch.Tensor(obs["observation"])
            goal_state = torch.Tensor(obs["desired_goal"])

            start_time = time.time()
            total_reward = 0
            crashed = False
            stuck = False
            trajectory = [initial_state.numpy()]

            for step in range(num_steps):
                action = planner(initial_state, goal_state, env.action_space.shape[0], env.observation_space.spaces["observation"].shape[0])
                obs, reward, done, truncated, info = env.step(action.numpy())
                total_reward += reward
                trajectory.append(torch.Tensor(obs["observation"]).numpy())

                if done:
                    break
                if truncated:
                    stuck = True
                    break
                if info.get('crashed', False):
                    crashed = True
                    break

                initial_state = torch.Tensor(obs["observation"])

            end_time = time.time()
            elapsed_time = end_time - start_time

            results.append({
                "Planner": planner_name,
                "Test Number": test_num + 1,
                "Total Reward": total_reward,
                "Elapsed Time": elapsed_time,
                "Crashed": crashed,
                "Stuck": stuck
            })

            plot_trajectory(trajectory, env, goal_state.numpy(), planner_name, test_num + 1)

    return results

def plot_trajectory(trajectory, env, goal, planner_name, test_num):
    plt.figure(figsize=(10, 10))
    scales = np.array(env.unwrapped.config["observation"]["scales"])
    trajectory = np.array(trajectory) * scales
    goal = goal * scales

    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', marker='.')
    plt.plot(goal[0] + 2.5 * np.array([-1, -1, 1, 1, -1]), goal[1] + 1.0 * np.array([-1, 1, 1, -1, -1]), 'r--')
    plt.plot(trajectory[0, 0] + 2.5 * np.array([-1, -1, 1, 1, -1]), trajectory[0, 1] + 1.0 * np.array([-1, 1, 1, -1, -1]), 'g-')
    plt.title(f"Trajectory for {planner_name}, Test {test_num}")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.grid(True)
    plt.show()

# Пример использования:
planners = {
    "CEM Planner": lambda state, goal, action_size, state_size: cem_planner(state, goal, action_size),
    "MPC Planner": lambda state, goal, action_size, state_size: mpc_planner(dynamics, state, goal),
    "ILQR Planner": lambda state, goal, action_size, state_size: ilqr_planner(state, goal, dynamics, action_size, state_size)
}

env = gym.make("parking-v0")
results = test_planners(planners, env)

# Вывод результатов в виде таблицы
import pandas as pd
df_results = pd.DataFrame(results)
print(df_results)

import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd

def set_env_state(env, observation, achieved_goal, desired_goal):
    env.reset()
    env.unwrapped.state = {
        'observation': observation.numpy(),
        'achieved_goal': achieved_goal.numpy(),
        'desired_goal': desired_goal.numpy()
    }

def plot_trajectory(trajectory, env, goal, planner_name, test_num, draw_interval=1):
    plt.figure(figsize=(10, 10))
    scales = np.array(env.unwrapped.config["observation"]["scales"])
    trajectory = np.array(trajectory) * scales
    goal = goal * scales

    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', marker='.')

    # Drawing the parking spot as a triangle
    parking_spot_length = 5.0
    parking_spot_width = 2.5
    goal_x, goal_y = goal[0], goal[1]
    goal_sin, goal_cos = goal[5], goal[4]
    parking_spot_orientation = np.arctan2(goal_sin, goal_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
    triangle = np.array([
        [0, -parking_spot_length / 2],
        [-parking_spot_width / 2, parking_spot_length / 2],
        [parking_spot_width / 2, parking_spot_length / 2],
    ])
    rotation_matrix = np.array([
        [np.cos(parking_spot_orientation), -np.sin(parking_spot_orientation)],
        [np.sin(parking_spot_orientation), np.cos(parking_spot_orientation)]
    ])
    triangle = triangle @ rotation_matrix.T + np.array([goal_x, goal_y])
    plt.plot([triangle[0, 0], triangle[1, 0], triangle[2, 0], triangle[0, 0]],
             [triangle[0, 1], triangle[1, 1], triangle[2, 1], triangle[0, 1]], 'r--')

    for i, state in enumerate(trajectory):
        if i % draw_interval == 0 or i == len(trajectory) - 1:
            # Drawing the car as a triangle
            car_x, car_y = state[0], state[1]
            car_sin, car_cos = state[5], state[4]
            car_yaw = np.arctan2(car_sin, car_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
            car_length = 4.8
            car_width = 2.0
            car_triangle = np.array([
                [0, -car_length / 2],
                [-car_width / 2, car_length / 2],
                [car_width / 2, car_length / 2],
            ])
            rotation_matrix = np.array([
                [np.cos(car_yaw), -np.sin(car_yaw)],
                [np.sin(car_yaw), np.cos(car_yaw)]
            ])
            car_triangle = car_triangle @ rotation_matrix.T + np.array([car_x, car_y])
            color = 'g-' if i != len(trajectory) - 1 else 'orange'
            plt.plot([car_triangle[0, 0], car_triangle[1, 0], car_triangle[2, 0], car_triangle[0, 0]],
                     [car_triangle[0, 1], car_triangle[1, 1], car_triangle[2, 1], car_triangle[0, 1]], color)

    plt.title(f"Trajectory for {planner_name}, Test {test_num}")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.xlim(-30, 30)
    plt.ylim(-20, 20)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.grid(True)
    plt.show()

def test_planners(planners, env, num_tests=5, num_steps=100, draw_interval=1):
    results = []
    initial_conditions = []
    for test_num in range(num_tests):
        obs, info = env.reset()
        initial_state = torch.Tensor(obs["observation"])
        achieved_goal = torch.Tensor(obs["achieved_goal"])
        desired_goal = torch.Tensor(obs["desired_goal"])
        initial_conditions.append((initial_state, achieved_goal, desired_goal))
    env.unwrapped.config["duration"] = num_steps
    for planner_name, planner in planners.items():
        for test_num, (initial_state, achieved_goal, desired_goal) in enumerate(initial_conditions):
            set_env_state(env, initial_state, achieved_goal, desired_goal)
            start_time = time.time()
            total_reward = 0
            crashed = False
            stuck = False
            trajectory = [initial_state.numpy()]
            done = False
            while not done:
                action = planner(initial_state, desired_goal, env.action_space.shape[0], env.observation_space.spaces["observation"].shape[0])
                obs, reward, done, truncated, info = env.step(action.numpy())
                total_reward += reward
                trajectory.append(torch.Tensor(obs["observation"]).numpy())
                if done:
                    break
                if truncated:
                    stuck = True
                    break
                if info.get('crashed', False):
                    crashed = True
                    break
                initial_state = torch.Tensor(obs["observation"])
            end_time = time.time()
            elapsed_time = end_time - start_time
            success = not crashed and not stuck and done
            results.append({
                "Planner": planner_name,
                "Test Number": test_num + 1,
                "Total Reward": total_reward,
                "Elapsed Time": elapsed_time,
                "Crashed": crashed,
                "Stuck": stuck,
                "Success": success
            })
            plot_trajectory(trajectory, env, desired_goal.numpy(), planner_name, test_num + 1, draw_interval)
    return results


planners = {
    "CEM Planner": lambda state, goal, action_size, state_size: cem_planner(state, goal, action_size),
    "MPC Planner": lambda state, goal, action_size, state_size: mpc_planner(dynamics, state, goal),
    "ILQR Planner": lambda state, goal, action_size, state_size: ilqr_planner(state, goal, dynamics, action_size, state_size)
}

env = gym.make("parking-v0", render_mode='rgb_array')
results = test_planners(planners, env, draw_interval=2)

# Вывод результатов в виде таблицы
df_results = pd.DataFrame(results)
print(df_results)

# Сохранение результатов в Excel файл
df_results.to_excel("test_results.xlsx", index=False)

import time
import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd
from tqdm import trange

def set_env_state(env, observation, achieved_goal, desired_goal):
    env.reset()
    env.unwrapped.state = {
        'observation': observation.numpy(),
        'achieved_goal': achieved_goal.numpy(),
        'desired_goal': desired_goal.numpy()
    }

def plot_trajectory(trajectory, env, goal, planner_name, test_num, draw_interval=1):
    plt.figure(figsize=(10, 10))
    scales = np.array(env.unwrapped.config["observation"]["scales"])
    trajectory = np.array(trajectory) * scales
    goal = goal * scales

    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', marker='.')

    # Drawing the parking spot as a triangle
    parking_spot_length = 5.0
    parking_spot_width = 2.5
    goal_x, goal_y = goal[0], goal[1]
    goal_sin, goal_cos = goal[5], goal[4]
    parking_spot_orientation = np.arctan2(goal_sin, goal_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
    triangle = np.array([
        [0, -parking_spot_length / 2],
        [-parking_spot_width / 2, parking_spot_length / 2],
        [parking_spot_width / 2, parking_spot_length / 2],
    ])
    rotation_matrix = np.array([
        [np.cos(parking_spot_orientation), -np.sin(parking_spot_orientation)],
        [np.sin(parking_spot_orientation), np.cos(parking_spot_orientation)]
    ])
    triangle = triangle @ rotation_matrix.T + np.array([goal_x, goal_y])
    plt.plot([triangle[0, 0], triangle[1, 0], triangle[2, 0], triangle[0, 0]],
             [triangle[0, 1], triangle[1, 1], triangle[2, 1], triangle[0, 1]], 'r--')

    for i, state in enumerate(trajectory):
        if (i!=0) and (i % draw_interval == 0 or i == len(trajectory) - 1):
            # Drawing the car as a triangle
            car_x, car_y = state[0], state[1]
            car_sin, car_cos = state[5], state[4]
            car_yaw = np.arctan2(car_sin, car_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
            car_length = 4.8
            car_width = 2.0
            car_triangle = np.array([
                [0, -car_length / 2],
                [-car_width / 2, car_length / 2],
                [car_width / 2, car_length / 2],
            ])
            rotation_matrix = np.array([
                [np.cos(car_yaw), -np.sin(car_yaw)],
                [np.sin(car_yaw), np.cos(car_yaw)]
            ])
            car_triangle = car_triangle @ rotation_matrix.T + np.array([car_x, car_y])
            color = 'g-' if i != len(trajectory) - 1 else 'orange'
            plt.plot([car_triangle[0, 0], car_triangle[1, 0], car_triangle[2, 0], car_triangle[0, 0]],
                     [car_triangle[0, 1], car_triangle[1, 1], car_triangle[2, 1], car_triangle[0, 1]], color)

    plt.title(f"Trajectory for {planner_name}, Test {test_num}")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.xlim(-30, 30)
    plt.ylim(-20, 20)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.grid(True)
    plt.show()

def test_planners(planners, env, num_tests=5, num_steps=100, draw_interval=1):
    results = []
    initial_conditions = []
    for test_num in range(num_tests):
        obs, info = env.reset()
        initial_state = torch.Tensor(obs["observation"])
        achieved_goal = torch.Tensor(obs["achieved_goal"])
        desired_goal = torch.Tensor(obs["desired_goal"])
        initial_conditions.append((initial_state, achieved_goal, desired_goal))
    env.unwrapped.config["duration"] = num_steps
    for planner_name, planner in planners.items():
        for test_num, (initial_state, achieved_goal, desired_goal) in enumerate(initial_conditions):
            set_env_state(env, initial_state, achieved_goal, desired_goal)
            start_time = time.time()
            total_reward = 0
            crashed = False
            stuck = False
            success = False
            trajectory = [initial_state.numpy()]
            done = False
            while not done:
                action = planner(initial_state, desired_goal, env.action_space.shape[0], env.observation_space.spaces["observation"].shape[0])
                # action = [0.001, 0.001]
                obs, reward, done, truncated, info = env.step(action.numpy())
                # obs, reward, done, truncated, info = env.step(action)
                total_reward += reward
                trajectory.append(torch.Tensor(obs["observation"]).numpy())
                if info.get('is_success', False):
                    success = True
                    break
                if info.get('crashed', False):
                    crashed = True
                    break
                if truncated:
                    stuck = True
                    break
                if done:
                    break

                initial_state = torch.Tensor(obs["observation"])
            end_time = time.time()
            elapsed_time = end_time - start_time
            results.append({
                "Planner": planner_name,
                "Test Number": test_num + 1,
                "Total Reward": total_reward,
                "Elapsed Time": elapsed_time,
                "Crashed": crashed,
                "Stuck": stuck,
                "Success": success
            })
            plot_trajectory(trajectory, env, desired_goal.numpy(), planner_name, test_num + 1, draw_interval)
    return results


planners = {
    "CEM Planner": lambda state, goal, action_size, state_size: cem_planner(state, goal, action_size),
    "MPC Planner": lambda state, goal, action_size, state_size: mpc_planner(dynamics, state, goal),
    "ILQR Planner": lambda state, goal, action_size, state_size: ilqr_planner(state, goal, dynamics, action_size, state_size)
}

env = gym.make("parking-v0", render_mode='rgb_array')
results = test_planners(planners, env, draw_interval=2)

# Вывод результатов в виде таблицы
df_results = pd.DataFrame(results)
print(df_results)

# Сохранение результатов в Excel файл
df_results.to_excel("test_results.xlsx", index=False)

import time
import matplotlib.pyplot as plt
import numpy as np
import torch
import pandas as pd
from tqdm import trange
import gymnasium as gym

def plot_trajectory(trajectory, env, goal, planner_name, test_num, draw_interval=1):
    plt.figure(figsize=(10, 10))
    scales = np.array(env.unwrapped.config["observation"]["scales"])
    trajectory = np.array(trajectory) * scales
    goal = goal * scales

    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', marker='.')

    # Drawing the parking spot as a triangle
    parking_spot_length = 5.0
    parking_spot_width = 2.5
    goal_x, goal_y = goal[0], goal[1]
    goal_sin, goal_cos = goal[5], goal[4]
    parking_spot_orientation = np.arctan2(goal_sin, goal_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
    triangle = np.array([
        [0, -parking_spot_length / 2],
        [-parking_spot_width / 2, parking_spot_length / 2],
        [parking_spot_width / 2, parking_spot_length / 2],
    ])
    rotation_matrix = np.array([
        [np.cos(parking_spot_orientation), -np.sin(parking_spot_orientation)],
        [np.sin(parking_spot_orientation), np.cos(parking_spot_orientation)]
    ])
    triangle = triangle @ rotation_matrix.T + np.array([goal_x, goal_y])
    plt.plot([triangle[0, 0], triangle[1, 0], triangle[2, 0], triangle[0, 0]],
             [triangle[0, 1], triangle[1, 1], triangle[2, 1], triangle[0, 1]], 'r--')

    for i, state in enumerate(trajectory):
        if (i != 0) and (i % draw_interval == 0 or i == len(trajectory) - 1):
            # Drawing the car as a triangle
            car_x, car_y = state[0], state[1]
            car_sin, car_cos = state[5], state[4]
            car_yaw = np.arctan2(car_sin, car_cos) + np.pi / 2  # Rotate by 90 degrees clockwise
            car_length = 4.8
            car_width = 2.0
            car_triangle = np.array([
                [0, -car_length / 2],
                [-car_width / 2, car_length / 2],
                [car_width / 2, car_length / 2],
            ])
            rotation_matrix = np.array([
                [np.cos(car_yaw), -np.sin(car_yaw)],
                [np.sin(car_yaw), np.cos(car_yaw)]
            ])
            car_triangle = car_triangle @ rotation_matrix.T + np.array([car_x, car_y])
            color = 'g-' if i != len(trajectory) - 1 else 'orange'
            plt.plot([car_triangle[0, 0], car_triangle[1, 0], car_triangle[2, 0], car_triangle[0, 0]],
                     [car_triangle[0, 1], car_triangle[1, 1], car_triangle[2, 1], car_triangle[0, 1]], color)

    plt.title(f"Trajectory for {planner_name}, Test {test_num}")
    plt.xlabel("X Position")
    plt.ylabel("Y Position")
    plt.xlim(-30, 30)
    plt.ylim(-20, 20)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.grid(True)
    plt.show()

def test_planners(planners, env_name, num_tests=5, draw_interval=1):
    results = []
    seeds = np.random.randint(0, 10000, size=num_tests)
    for planner_name, planner in planners.items():
        for test_num in range(num_tests):
            env = gym.make(env_name)
            env.action_space.seed(int(seeds[test_num]))
            obs, info = env.reset(seed=int(seeds[test_num]))
            initial_state = torch.Tensor(obs["observation"])
            achieved_goal = torch.Tensor(obs["achieved_goal"])
            desired_goal = torch.Tensor(obs["desired_goal"])
            start_time = time.time()
            total_reward = 0
            crashed = False
            stuck = False
            success = False
            trajectory = [initial_state.numpy()]
            done = False
            while not done:
                action = planner(initial_state, desired_goal, env.action_space.shape[0], env.observation_space.spaces["observation"].shape[0])
                obs, reward, done, truncated, info = env.step(action.numpy())
                total_reward += reward
                trajectory.append(torch.Tensor(obs["observation"]).numpy())
                if info.get('is_success', False):
                    success = True
                    done = True
                if info.get('crashed', False):
                    crashed = True
                    done = True
                if truncated:
                    stuck = True
                    done = True
                if done:
                    break

                initial_state = torch.Tensor(obs["observation"])
            end_time = time.time()
            elapsed_time = end_time - start_time
            results.append({
                "Planner": planner_name,
                "Test Number": test_num + 1,
                "Total Reward": total_reward,
                "Elapsed Time": elapsed_time,
                "Not crashed": not crashed,
                "Not stuck": not stuck,
                "Success": success
            })
            plot_trajectory(trajectory, env, desired_goal.numpy(), planner_name, test_num + 1, draw_interval)
    return results

planners = {
    "CEM Planner": lambda state, goal, action_size, state_size: cem_planner(state, goal, action_size),
    "MPC Planner": lambda state, goal, action_size, state_size: mpc_planner(dynamics, state, goal),
    "ILQR Planner": lambda state, goal, action_size, state_size: ilqr_planner(state, goal, dynamics, action_size, state_size)
}

env_name = "parking-v0"
results = test_planners(planners, env_name, draw_interval=2)

# Вывод результатов в виде таблицы
df_results = pd.DataFrame(results)
print(df_results)

# Сохранение результатов в Excel файл
df_results.to_excel("test_results.xlsx", index=False)

import gymnasium as gym
import highway_env
import torch

# Создание окружения
env = gym.make("parking-v0", render_mode="rgb_array")
obs, info = env.reset()

# Начальное состояние
initial_state = torch.Tensor(obs["observation"])
achieved_goal = torch.Tensor(obs["achieved_goal"])
desired_goal = torch.Tensor(obs["desired_goal"])

# Задаем действия для движения вперед (газ на максимум, угол рулевого колеса - 0)
action = [.001, 0.001]  # Максимальное ускорение и нулевой угол рулевого колеса

# Максимальное количество шагов
max_steps = 100
env.unwrapped.config["duration"] = 98
# Запуск цикла для проверки информации при каждом шаге
for step in range(max_steps):
    obs, reward, done, truncated, info = env.step(action)
    print(f"Step: {step + 1}")
    print(f"Info: {info}", end = " ")
    print(f"truncted: {truncated}", end = " ")
    print(f"Done: {done}")

    if done or truncated:
        print("Episode ended.")
        break

env.close()


# Make the environment, and run an episode with random actions:
env = gym.make("parking-v0", render_mode="rgb_array")
env = record_videos(env)
obs, info = env.reset()

k=0
done = False
while not done:
    action = ilqr_planner(torch.Tensor(obs["observation"]), torch.Tensor(obs["desired_goal"]), dynamics, env.action_space.shape[0], env.observation_space.spaces["observation"].shape[0])
    k=k+1
    obs, reward, done, truncated, info = env.step(action)
    if k==100:
      done=True
env.close()
show_videos()
